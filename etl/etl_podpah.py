''"""
ETL Projeto Podpah - YouTube Data Pipeline
Vers√£o: 3.0 (An√°lise Temporal - Semestral/Anual)
"""

import os
import sys
import time
import logging
import pandas as pd
import sqlalchemy
import sqlite3
import isodate
from datetime import datetime, timedelta
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError
from dotenv import load_dotenv
from tqdm import tqdm
from typing import List, Dict, Optional

# ========================================
# CONFIGURA√á√ÉO DE LOGGING
# ========================================
def setup_logging():
    """Configura logging profissional."""
    log_folder = 'logs'
    os.makedirs(log_folder, exist_ok=True)
    
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    log_file = os.path.join(log_folder, f'etl_{timestamp}.log')
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s [%(levelname)s] %(message)s',
        handlers=[
            logging.FileHandler(log_file, encoding='utf-8'),
            logging.StreamHandler(sys.stdout)
        ]
    )
    return logging.getLogger(__name__)


# ========================================
# CONFIGURA√á√ïES
# ========================================
class Config:
    """Configura√ß√£o centralizada."""
    
    def __init__(self):
        load_dotenv()
        self.api_key = os.getenv('YOUTUBE_API_KEY')
        self.channel_id = os.getenv('CHANNEL_ID', 'UCj9R9rOhl81fhnKxBpwJ-yw')
        self.db_path = 'data/banco_youtube.db'
        self.csv_output = 'data/dados_youtube_atual.csv'
        self.backup_folder = 'backups'
        self.max_retries = 3
        self.retry_delay = 5
        self.batch_size = 50
        self.quota_diaria = 10000  # Quota gratuita do YouTube
        
        self._validate()
    
    def _validate(self):
        if not self.api_key:
            raise ValueError("‚ùå ERRO: YOUTUBE_API_KEY n√£o encontrada no .env")
        
        os.makedirs('data', exist_ok=True)
        os.makedirs(self.backup_folder, exist_ok=True)


# ========================================
# QUOTA TRACKER
# ========================================
class QuotaTracker:
    """Rastreador de consumo de quota da API."""
    
    CUSTOS = {
        'channels.list': 1,
        'playlistItems.list': 1,
        'videos.list': 1
    }
    
    def __init__(self, quota_diaria: int = 10000):
        self.quota_diaria = quota_diaria
        self.quota_usada = 0
        self.chamadas_detalhadas = []
    
    def registrar(self, tipo: str, quantidade: int = 1):
        """Registra chamada de API."""
        custo = self.CUSTOS.get(tipo, 1) * quantidade
        self.quota_usada += custo
        self.chamadas_detalhadas.append({
            'tipo': tipo,
            'quantidade': quantidade,
            'custo': custo,
            'timestamp': datetime.now()
        })
    
    def get_percentual(self) -> float:
        """Retorna percentual de quota usada."""
        return (self.quota_usada / self.quota_diaria) * 100
    
    def get_alerta(self) -> str:
        """Retorna alerta baseado no consumo."""
        perc = self.get_percentual()
        
        if perc >= 90:
            return f"üö® CR√çTICO: {perc:.1f}% da quota di√°ria!"
        elif perc >= 70:
            return f"‚ö†Ô∏è  ATEN√á√ÉO: {perc:.1f}% da quota usada"
        else:
            return f"‚úÖ NORMAL: {perc:.1f}% da quota usada"
    
    def relatorio(self) -> str:
        """Gera relat√≥rio de quota."""
        return f"""
{'='*60}
üìä CONSUMO DE QUOTA API
{'='*60}
üî¢ Quota usada: {self.quota_usada:,} / {self.quota_diaria:,} unidades
üìà Percentual: {self.get_percentual():.2f}%
üì° Total de chamadas: {len(self.chamadas_detalhadas)}
‚ö° Quota restante: {self.quota_diaria - self.quota_usada:,} unidades

{self.get_alerta()}
{'='*60}
"""


# ========================================
# FUN√á√ïES AUXILIARES
# ========================================
def retry_on_error(max_retries: int = 3, delay: int = 5):
    """Decorator para retry autom√°tico."""
    def decorator(func):
        def wrapper(*args, **kwargs):
            for attempt in range(max_retries):
                try:
                    return func(*args, **kwargs)
                except HttpError as e:
                    if e.resp.status == 403:
                        logger.error(f"‚ùå Quota da API excedida: {e}")
                        raise
                    
                    wait_time = delay * (2 ** attempt)
                    logger.warning(f"‚ö†Ô∏è  Tentativa {attempt + 1}/{max_retries} falhou. "
                                 f"Aguardando {wait_time}s...")
                    time.sleep(wait_time)
                    
                    if attempt == max_retries - 1:
                        logger.error(f"‚ùå Falha ap√≥s {max_retries} tentativas")
                        raise
                except Exception as e:
                    logger.error(f"‚ùå Erro inesperado: {e}")
                    raise
        return wrapper
    return decorator


def validate_dataframe(df: pd.DataFrame) -> bool:
    """Valida schema e qualidade dos dados."""
    required_columns = ['Video_ID', 'Titulo', 'Data_Publicacao', 'Views', 
                       'Likes', 'Comentarios', 'Thumbnail_URL']
    
    missing_cols = set(required_columns) - set(df.columns)
    if missing_cols:
        logger.error(f"‚ùå Colunas faltando: {missing_cols}")
        return False
    
    if df['Video_ID'].duplicated().any():
        logger.warning("‚ö†Ô∏è  IDs duplicados. Removendo...")
        df.drop_duplicates(subset='Video_ID', inplace=True)
    
    if df['Views'].isnull().any():
        logger.warning("‚ö†Ô∏è  Views nulas. Preenchendo com 0...")
        df['Views'].fillna(0, inplace=True)
    
    logger.info(f"‚úÖ Valida√ß√£o OK: {len(df)} registros v√°lidos")
    return True


# ========================================
# CLASSE PRINCIPAL ETL
# ========================================
class YouTubeETL:
    """Pipeline ETL com hist√≥rico temporal."""
    
    def __init__(self, config: Config):
        self.config = config
        self.youtube = build('youtube', 'v3', developerKey=config.api_key)
        self.quota_tracker = QuotaTracker(config.quota_diaria)
        self.stats = {
            'videos_coletados': 0,
            'tempo_inicio': datetime.now(),
            'data_execucao': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }
    
    @retry_on_error(max_retries=3, delay=5)
    def get_channel_info(self) -> str:
        """Busca informa√ß√µes do canal."""
        logger.info("üîç Buscando informa√ß√µes do canal...")
        
        request = self.youtube.channels().list(
            part='contentDetails,statistics,snippet',
            id=self.config.channel_id
        )
        response = request.execute()
        self.quota_tracker.registrar('channels.list')
        
        if not response.get('items'):
            raise ValueError(f"Canal n√£o encontrado: {self.config.channel_id}")
        
        channel_data = response['items'][0]
        uploads_id = channel_data['contentDetails']['relatedPlaylists']['uploads']
        subs_count = int(channel_data['statistics']['subscriberCount'])
        channel_name = channel_data['snippet']['title']
        
        logger.info(f"üì∫ Canal: {channel_name}")
        logger.info(f"üë• Inscritos: {subs_count:,}")
        
        return uploads_id
    
    @retry_on_error(max_retries=3, delay=5)
    def get_all_video_ids(self, uploads_id: str) -> List[str]:
        """Coleta todos os IDs de v√≠deos."""
        video_ids = []
        next_page_token = None
        
        logger.info("üîç Coletando lista de v√≠deos...")
        
        with tqdm(desc="P√°ginas processadas", unit="p√°g") as pbar:
            while True:
                request = self.youtube.playlistItems().list(
                    part='contentDetails',
                    playlistId=uploads_id,
                    maxResults=self.config.batch_size,
                    pageToken=next_page_token
                )
                response = request.execute()
                self.quota_tracker.registrar('playlistItems.list')
                
                for item in response['items']:
                    video_ids.append(item['contentDetails']['videoId'])
                
                pbar.update(1)
                pbar.set_postfix({'V√≠deos': len(video_ids)})
                
                next_page_token = response.get('nextPageToken')
                if not next_page_token:
                    break
        
        logger.info(f"‚úÖ Total de v√≠deos: {len(video_ids)}")
        self.stats['videos_coletados'] = len(video_ids)
        return video_ids
    
    @retry_on_error(max_retries=3, delay=5)
    def get_video_details(self, video_ids: List[str]) -> pd.DataFrame:
        """Busca detalhes completos dos v√≠deos."""
        video_data = []
        total_batches = (len(video_ids) + self.config.batch_size - 1) // self.config.batch_size
        
        logger.info(f"üìä Coletando detalhes ({total_batches} lotes)...")
        
        with tqdm(total=len(video_ids), desc="V√≠deos processados", unit="v√≠deo") as pbar:
            for i in range(0, len(video_ids), self.config.batch_size):
                batch_ids = video_ids[i:i + self.config.batch_size]
                ids_string = ','.join(batch_ids)
                
                request = self.youtube.videos().list(
                    part='snippet,statistics,contentDetails',
                    id=ids_string
                )
                response = request.execute()
                self.quota_tracker.registrar('videos.list')
                
                for item in response['items']:
                    try:
                        video_data.append(self._parse_video_item(item))
                    except Exception as e:
                        logger.warning(f"‚ö†Ô∏è  Erro ao processar {item['id']}: {e}")
                        continue
                
                pbar.update(len(batch_ids))
        
        return pd.DataFrame(video_data)
    
    def _parse_video_item(self, item: Dict) -> Dict:
        """Extrai dados de um v√≠deo."""
        stats = item.get('statistics', {})
        snippet = item.get('snippet', {})
        content_details = item.get('contentDetails', {})
        
        # Dura√ß√£o
        duracao_iso = content_details.get('duration', 'PT0S')
        try:
            duracao_segundos = int(isodate.parse_duration(duracao_iso).total_seconds())
        except:
            duracao_segundos = 0
        
        # Thumbnail
        thumbnails = snippet.get('thumbnails', {})
        thumbnail_url = (
            thumbnails.get('maxres', {}).get('url') or
            thumbnails.get('high', {}).get('url') or
            thumbnails.get('medium', {}).get('url') or
            thumbnails.get('default', {}).get('url') or
            f"https://img.youtube.com/vi/{item['id']}/hqdefault.jpg"
        )
        
        return {
            'Video_ID': item['id'],
            'Titulo': snippet.get('title', 'Sem t√≠tulo'),
            'Data_Publicacao': snippet.get('publishedAt'),
            'Views': int(stats.get('viewCount', 0)),
            'Likes': int(stats.get('likeCount', 0)),
            'Comentarios': int(stats.get('commentCount', 0)),
            'Duracao_ISO': duracao_iso,
            'Duracao_Segundos': duracao_segundos,
            'Duracao_Formatada': str(timedelta(seconds=duracao_segundos)),
            'Thumbnail_URL': thumbnail_url,
            'Data_Coleta': self.stats['data_execucao']
        }
    
    def save_data(self, df: pd.DataFrame):
        """Salva dados com hist√≥rico temporal."""
        if not validate_dataframe(df):
            raise ValueError("Dados inv√°lidos. Opera√ß√£o cancelada.")
        
        df['Data_Publicacao'] = pd.to_datetime(df['Data_Publicacao'])
        df['Data_Simples'] = df['Data_Publicacao'].dt.date
        
        # CSV snapshot atual
        df.to_csv(self.config.csv_output, index=False)
        logger.info(f"‚úÖ CSV salvo: {self.config.csv_output}")
        
        # Backup CSV com timestamp
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        backup_csv = os.path.join(self.config.backup_folder, f"snapshot_{timestamp}.csv")
        df.to_csv(backup_csv, index=False)
        logger.info(f"üíæ Backup salvo: {backup_csv}")
        
        # SQLite com estrutura temporal
        engine = sqlalchemy.create_engine(f'sqlite:///{self.config.db_path}')
        
        # Tabela snapshot atual (sempre substitui)
        df.to_sql('videos_stats_atual', engine, if_exists='replace', index=False)
        
        # Tabela hist√≥rico (acumula todas execu√ß√µes)
        df.to_sql('videos_historico', engine, if_exists='append', index=False)
        
        # Log de execu√ß√£o
        self._save_execution_log(engine, df)
        
        logger.info(f"‚úÖ Banco atualizado: {self.config.db_path}")
        logger.info(f"   üìä Snapshot atual: {len(df)} registros")
        logger.info(f"   üìà Hist√≥rico: Dados acumulados")
    
    def _save_execution_log(self, engine, df: pd.DataFrame):
        """Salva log de execu√ß√£o no banco."""
        tempo_total = (datetime.now() - self.stats['tempo_inicio']).total_seconds()
        
        log_data = pd.DataFrame([{
            'Data_Execucao': self.stats['data_execucao'],
            'Videos_Coletados': len(df),
            'Chamadas_API': len(self.quota_tracker.chamadas_detalhadas),
            'Quota_Usada': self.quota_tracker.quota_usada,
            'Quota_Percentual': self.quota_tracker.get_percentual(),
            'Tempo_Execucao_Segundos': tempo_total,
            'Total_Views': df['Views'].sum(),
            'Total_Likes': df['Likes'].sum(),
            'Total_Comentarios': df['Comentarios'].sum()
        }])
        
        log_data.to_sql('execucoes_log', engine, if_exists='append', index=False)
    
    def generate_report(self):
        """Gera relat√≥rio completo de execu√ß√£o."""
        tempo_total = (datetime.now() - self.stats['tempo_inicio']).total_seconds()
        
        report = f"""
{'='*60}
üéØ RELAT√ìRIO DE EXECU√á√ÉO - ETL PODPAH
{'='*60}
üìÖ Data: {self.stats['data_execucao']}
‚è±Ô∏è  Tempo total: {tempo_total:.2f}s
üìπ V√≠deos coletados: {self.stats['videos_coletados']:,}
‚ö° M√©dia: {tempo_total/max(self.stats['videos_coletados'], 1):.2f}s/v√≠deo
{'='*60}
"""
        logger.info(report)
        logger.info(self.quota_tracker.relatorio())


# ========================================
# INICIALIZA√á√ÉO DO BANCO
# ========================================
def init_database(db_path: str):
    """Cria estrutura do banco na primeira execu√ß√£o."""
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    
    # Verifica se tabelas existem antes de criar √≠ndices
    cursor.execute("""
        SELECT name FROM sqlite_master 
        WHERE type='table' AND name='videos_historico'
    """)
    
    if cursor.fetchone():
        # S√≥ cria √≠ndices se a tabela j√° existir
        cursor.execute("""
            CREATE INDEX IF NOT EXISTS idx_video_id 
            ON videos_historico(Video_ID)
        """)
        
        cursor.execute("""
            CREATE INDEX IF NOT EXISTS idx_data_coleta 
            ON videos_historico(Data_Coleta)
        """)
        
        logger.info("‚úÖ √çndices do banco criados/verificados")
    
    conn.commit()
    conn.close()


# ========================================
# EXECU√á√ÉO PRINCIPAL
# ========================================
def main():
    """Execu√ß√£o principal do ETL."""
    global logger
    logger = setup_logging()
    
    try:
        logger.info("="*60)
        logger.info("üöÄ ETL PODPAH - AN√ÅLISE TEMPORAL (SEMESTRAL/ANUAL)")
        logger.info("="*60)
        
        config = Config()
        etl = YouTubeETL(config)
        
        # Inicializa banco
        init_database(config.db_path)
        
        # Pipeline ETL
        uploads_id = etl.get_channel_info()
        video_ids = etl.get_all_video_ids(uploads_id)
        df_videos = etl.get_video_details(video_ids)
        etl.save_data(df_videos)
        
        etl.generate_report()
        
        logger.info("="*60)
        logger.info("‚úÖ ETL CONCLU√çDO COM SUCESSO!")
        logger.info("="*60)
        logger.info("\nüí° PR√ìXIMA EXECU√á√ÉO:")
        logger.info("   üìÖ Sugerido: 6 meses (an√°lise semestral)")
        logger.info("   üìä Dados hist√≥ricos preservados para compara√ß√£o")
        
    except Exception as e:
        logger.error(f"‚ùå ERRO CR√çTICO: {e}", exc_info=True)
        sys.exit(1)


if __name__ == "__main__":
    main()